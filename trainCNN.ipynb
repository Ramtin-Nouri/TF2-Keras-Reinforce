{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "\n",
    "import os,cv2, datetime,numpy as np\n",
    "from tensorflow.keras import callbacks\n",
    "import dataManager\n",
    "from nets import conv as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "So first lets get the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 80, 80, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 40, 40, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 229,441\n",
      "Trainable params: 229,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = net.NeuralNetwork()\n",
    "model,_ = nn.getModel((80,80,1),(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "Now lets get the Pong environment from OpenAi Gym and define the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym initialization\n",
    "env = dataManager.SingleGym(\"Pong-v0\",True,use_diff=True,stack=False)\n",
    "observation = env.reset()\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "to keep track of the progress and save our model we define some callbacks that will be called after the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "today = datetime.datetime.today()\n",
    "time = \"%s%04d-%02d-%02d-%02d-%02d-%02d/\" % (nn.getModelFolderPath(),today.year,today.month,today.day,today.hour,today.minute,today.second)\n",
    "os.makedirs(\"%s/figs\"%(time))\n",
    "\n",
    "with open('%s/architecture.txt'%(time),'w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "csvLogger = callbacks.CSVLogger(time+\"log.csv\", separator=',', append=False)\n",
    "tensorboardCallback = callbacks.TensorBoard(log_dir=time)#TODO: load test data in advance and use histogram_freq=1 here\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(time)\n",
    "file_writer.set_as_default()\n",
    "\n",
    "\n",
    "callbacks =  [callbacks.ModelCheckpoint(time+\"{epoch:04d}.hdf5\",\n",
    "monitor='val_loss',verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=5),\n",
    "csvLogger,tensorboardCallback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Train\n",
    "Now to the real training.  \n",
    "\n",
    "Given an observation first we sample an action, where we randomly take either value predicted by the network or a random action. The higher the networks confidence the higher the probability that we chose its action over the random action.\n",
    "\n",
    "We then add the observation to our training input and the action to the labels.  \n",
    "\n",
    "Now we take the action chosen in the environment and get the new observation and reward.  \n",
    "This will be repeated until the game (aka. an episode) is over.\n",
    "\n",
    "Now we'll calulate the discounted rewards and train our model using the gathered training input and labels and using the discounted rewards as sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode = 0\n",
    "running_reward = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    upProbability = model.predict(np.array([observation]))[0]\n",
    "    if np.random.uniform() < upProbability:\n",
    "        action = UP_ACTION\n",
    "    else:\n",
    "        action = DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append([observation])\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    #env.env.render()\n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode, 'the total reward was :', reward_sum)\n",
    "        \n",
    "        # training\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=2, callbacks=callbacks, sample_weight=dataManager.calculateRewards(rewards, gamma),initial_epoch = episode, epochs = episode+1)\n",
    "        \n",
    "        running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "        #Log Running Reward\n",
    "        tf.summary.scalar(\"Running Reward\", running_reward,step=episode)\n",
    "        tf.summary.scalar(\"Reward Sum\", reward_sum,step=episode)\n",
    "        \n",
    "        \n",
    "        # increment episode number\n",
    "        episode += 1\n",
    "        \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
