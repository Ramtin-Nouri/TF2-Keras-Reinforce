{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "\n",
    "import os,cv2, datetime,numpy as np\n",
    "from tensorflow.keras import callbacks\n",
    "import dataManager\n",
    "from nets import conv as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 210, 160, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 105, 80, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 105, 80, 16)       1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 52, 40, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1065088   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,089,745\n",
      "Trainable params: 1,089,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = net.NeuralNetwork()\n",
    "model,_ = nn.getModel((210, 160, 3),(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym initialization\n",
    "env = dataManager.SingleGym(\"Pong-v0\",False)\n",
    "observation = env.reset()\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "today = datetime.datetime.today()\n",
    "time = \"%s%04d-%02d-%02d-%02d-%02d-%02d/\" % (nn.getModelFolderPath(),today.year,today.month,today.day,today.hour,today.minute,today.second)\n",
    "os.makedirs(\"%s/figs\"%(time))\n",
    "\n",
    "with open('%s/architecture.txt'%(time),'w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "csvLogger = callbacks.CSVLogger(time+\"log.csv\", separator=',', append=False)\n",
    "tensorboardCallback = callbacks.TensorBoard(log_dir=time)#TODO: load test data in advance and use histogram_freq=1 here\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(time)\n",
    "file_writer.set_as_default()\n",
    "\n",
    "\n",
    "callbacks =  [callbacks.ModelCheckpoint(time+\"{epoch:04d}.hdf5\",\n",
    "monitor='val_loss',verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=5),\n",
    "csvLogger,tensorboardCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -21.0\n",
      "34/34 [==============================] - 4s 36ms/step - loss: -3.9659e-04\n",
      "At the end of episode 1 the total reward was : -21.0\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 0.0014\n",
      "At the end of episode 2 the total reward was : -21.0\n",
      "Epoch 3/3\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 0.0062\n",
      "At the end of episode 3 the total reward was : -21.0\n",
      "Epoch 4/4\n",
      "38/38 [==============================] - 1s 29ms/step - loss: -7.6043e-04\n",
      "At the end of episode 4 the total reward was : -21.0\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.0036\n",
      "\n",
      "Epoch 00005: saving model to saveData/CNN/2021-05-31-22-30-05/0005.hdf5\n",
      "At the end of episode 5 the total reward was : -21.0\n",
      "Epoch 6/6\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 9.8908e-04\n",
      "At the end of episode 6 the total reward was : -21.0\n",
      "Epoch 7/7\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.0022\n",
      "At the end of episode 7 the total reward was : -21.0\n",
      "Epoch 8/8\n",
      "42/42 [==============================] - 1s 30ms/step - loss: -0.0016\n",
      "At the end of episode 8 the total reward was : -21.0\n",
      "Epoch 9/9\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0026\n",
      "At the end of episode 9 the total reward was : -20.0\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 1s 28ms/step - loss: -0.0089\n",
      "\n",
      "Epoch 00010: saving model to saveData/CNN/2021-05-31-22-30-05/0010.hdf5\n",
      "At the end of episode 10 the total reward was : -21.0\n",
      "Epoch 11/11\n",
      "34/34 [==============================] - 1s 26ms/step - loss: 0.0019\n",
      "At the end of episode 11 the total reward was : -20.0\n",
      "Epoch 12/12\n",
      "41/41 [==============================] - 1s 29ms/step - loss: 0.0018\n",
      "At the end of episode 12 the total reward was : -21.0\n",
      "Epoch 13/13\n",
      "35/35 [==============================] - 1s 25ms/step - loss: 0.0066\n",
      "At the end of episode 13 the total reward was : -21.0\n",
      "Epoch 14/14\n",
      "51/51 [==============================] - 1s 26ms/step - loss: -0.0179\n",
      "At the end of episode 14 the total reward was : -21.0\n",
      "Epoch 15/15\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0130\n",
      "\n",
      "Epoch 00015: saving model to saveData/CNN/2021-05-31-22-30-05/0015.hdf5\n",
      "At the end of episode 15 the total reward was : -21.0\n",
      "Epoch 16/16\n",
      "40/40 [==============================] - 1s 33ms/step - loss: -0.0070\n",
      "At the end of episode 16 the total reward was : -21.0\n",
      "Epoch 17/17\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 0.0061\n",
      "At the end of episode 17 the total reward was : -21.0\n",
      "Epoch 18/18\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 0.0035\n",
      "At the end of episode 18 the total reward was : -20.0\n",
      "Epoch 19/19\n",
      "59/59 [==============================] - 1s 21ms/step - loss: -0.0030\n",
      "At the end of episode 19 the total reward was : -21.0\n",
      "Epoch 20/20\n",
      "48/48 [==============================] - 1s 23ms/step - loss: -0.0013\n",
      "\n",
      "Epoch 00020: saving model to saveData/CNN/2021-05-31-22-30-05/0020.hdf5\n",
      "At the end of episode 20 the total reward was : -21.0\n",
      "Epoch 21/21\n",
      "40/40 [==============================] - 1s 28ms/step - loss: -6.5149e-04\n",
      "At the end of episode 21 the total reward was : -20.0\n",
      "Epoch 22/22\n",
      "44/44 [==============================] - 1s 30ms/step - loss: -0.0103\n",
      "At the end of episode 22 the total reward was : -20.0\n",
      "Epoch 23/23\n",
      "42/42 [==============================] - 1s 24ms/step - loss: -0.0061\n",
      "At the end of episode 23 the total reward was : -21.0\n",
      "Epoch 24/24\n",
      "45/45 [==============================] - 1s 32ms/step - loss: -0.0121\n",
      "At the end of episode 24 the total reward was : -21.0\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 4.4162e-04\n",
      "\n",
      "Epoch 00025: saving model to saveData/CNN/2021-05-31-22-30-05/0025.hdf5\n",
      "At the end of episode 25 the total reward was : -16.0\n",
      "Epoch 26/26\n",
      "65/65 [==============================] - 2s 23ms/step - loss: -0.0034\n",
      "At the end of episode 26 the total reward was : -19.0\n",
      "Epoch 27/27\n",
      "58/58 [==============================] - 1s 21ms/step - loss: 1.9948e-04\n",
      "At the end of episode 27 the total reward was : -20.0\n",
      "Epoch 28/28\n",
      "45/45 [==============================] - 1s 23ms/step - loss: -0.0051\n",
      "At the end of episode 28 the total reward was : -19.0\n",
      "Epoch 29/29\n",
      "69/69 [==============================] - 1s 20ms/step - loss: -0.0068\n",
      "At the end of episode 29 the total reward was : -18.0\n",
      "Epoch 30/30\n",
      "62/62 [==============================] - 2s 25ms/step - loss: -0.0214\n",
      "\n",
      "Epoch 00030: saving model to saveData/CNN/2021-05-31-22-30-05/0030.hdf5\n",
      "At the end of episode 30 the total reward was : -21.0\n",
      "Epoch 31/31\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 0.0080\n",
      "At the end of episode 31 the total reward was : -19.0\n",
      "Epoch 32/32\n",
      "81/81 [==============================] - 2s 21ms/step - loss: -0.0098\n",
      "At the end of episode 32 the total reward was : -20.0\n",
      "Epoch 33/33\n",
      "46/46 [==============================] - 1s 23ms/step - loss: -0.0092\n",
      "At the end of episode 33 the total reward was : -21.0\n",
      "Epoch 34/34\n",
      "40/40 [==============================] - 1s 23ms/step - loss: 0.0218\n",
      "At the end of episode 34 the total reward was : -20.0\n",
      "Epoch 35/35\n",
      "51/51 [==============================] - 1s 27ms/step - loss: -0.0203\n",
      "\n",
      "Epoch 00035: saving model to saveData/CNN/2021-05-31-22-30-05/0035.hdf5\n",
      "At the end of episode 35 the total reward was : -20.0\n",
      "Epoch 36/36\n",
      "54/54 [==============================] - 2s 28ms/step - loss: 0.0204\n",
      "At the end of episode 36 the total reward was : -20.0\n",
      "Epoch 37/37\n",
      "62/62 [==============================] - 1s 24ms/step - loss: -0.0040\n",
      "At the end of episode 37 the total reward was : -20.0\n",
      "Epoch 38/38\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0263\n",
      "At the end of episode 38 the total reward was : -20.0\n",
      "Epoch 39/39\n",
      "54/54 [==============================] - 2s 29ms/step - loss: -0.0109\n",
      "At the end of episode 39 the total reward was : -21.0\n",
      "Epoch 40/40\n",
      "53/53 [==============================] - 1s 22ms/step - loss: 0.0038\n",
      "\n",
      "Epoch 00040: saving model to saveData/CNN/2021-05-31-22-30-05/0040.hdf5\n",
      "At the end of episode 40 the total reward was : -21.0\n",
      "Epoch 41/41\n",
      "63/63 [==============================] - 1s 23ms/step - loss: -0.0013\n",
      "At the end of episode 41 the total reward was : -20.0\n",
      "Epoch 42/42\n",
      "56/56 [==============================] - 2s 29ms/step - loss: -0.0068\n",
      "At the end of episode 42 the total reward was : -20.0\n",
      "Epoch 43/43\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 0.0170\n",
      "At the end of episode 43 the total reward was : -20.0\n",
      "Epoch 44/44\n",
      "51/51 [==============================] - 2s 36ms/step - loss: -0.0012\n",
      "At the end of episode 44 the total reward was : -20.0\n",
      "Epoch 45/45\n",
      "72/72 [==============================] - 2s 30ms/step - loss: -0.0075\n",
      "\n",
      "Epoch 00045: saving model to saveData/CNN/2021-05-31-22-30-05/0045.hdf5\n",
      "At the end of episode 45 the total reward was : -18.0\n",
      "Epoch 46/46\n",
      "62/62 [==============================] - 2s 27ms/step - loss: -0.0013\n",
      "At the end of episode 46 the total reward was : -20.0\n",
      "Epoch 47/47\n",
      "74/74 [==============================] - 2s 21ms/step - loss: -2.4336e-04\n",
      "At the end of episode 47 the total reward was : -21.0\n",
      "Epoch 48/48\n",
      "55/55 [==============================] - 1s 25ms/step - loss: -0.0255\n",
      "At the end of episode 48 the total reward was : -21.0\n",
      "Epoch 49/49\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 2.1781e-04\n",
      "At the end of episode 49 the total reward was : -16.0\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 0.0019\n",
      "\n",
      "Epoch 00050: saving model to saveData/CNN/2021-05-31-22-30-05/0050.hdf5\n",
      "At the end of episode 50 the total reward was : -19.0\n",
      "Epoch 51/51\n",
      "81/81 [==============================] - 2s 20ms/step - loss: -0.0132\n",
      "At the end of episode 51 the total reward was : -21.0\n",
      "Epoch 52/52\n",
      "40/40 [==============================] - 1s 24ms/step - loss: -0.0680\n",
      "At the end of episode 52 the total reward was : -21.0\n",
      "Epoch 53/53\n",
      "50/50 [==============================] - 1s 22ms/step - loss: -0.0061\n",
      "At the end of episode 53 the total reward was : -20.0\n",
      "Epoch 54/54\n",
      "66/66 [==============================] - 1s 21ms/step - loss: -0.0096\n",
      "At the end of episode 54 the total reward was : -19.0\n",
      "Epoch 55/55\n",
      "58/58 [==============================] - 1s 25ms/step - loss: -0.0093\n",
      "\n",
      "Epoch 00055: saving model to saveData/CNN/2021-05-31-22-30-05/0055.hdf5\n",
      "At the end of episode 55 the total reward was : -21.0\n",
      "Epoch 56/56\n",
      "37/37 [==============================] - 1s 24ms/step - loss: -0.0082\n",
      "At the end of episode 56 the total reward was : -17.0\n",
      "Epoch 57/57\n",
      "59/59 [==============================] - 1s 21ms/step - loss: -0.0466\n",
      "At the end of episode 57 the total reward was : -19.0\n",
      "Epoch 58/58\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 2.4471e-04\n",
      "At the end of episode 58 the total reward was : -19.0\n",
      "Epoch 59/59\n",
      "45/45 [==============================] - 1s 23ms/step - loss: -0.0090\n",
      "At the end of episode 59 the total reward was : -21.0\n",
      "Epoch 60/60\n",
      "45/45 [==============================] - 1s 23ms/step - loss: -0.0324\n",
      "\n",
      "Epoch 00060: saving model to saveData/CNN/2021-05-31-22-30-05/0060.hdf5\n",
      "At the end of episode 60 the total reward was : -21.0\n",
      "Epoch 61/61\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.0196\n",
      "At the end of episode 61 the total reward was : -20.0\n",
      "Epoch 62/62\n",
      "68/68 [==============================] - 1s 21ms/step - loss: -0.0023\n",
      "At the end of episode 62 the total reward was : -21.0\n",
      "Epoch 63/63\n",
      "51/51 [==============================] - 1s 22ms/step - loss: -0.0299\n",
      "At the end of episode 63 the total reward was : -21.0\n",
      "Epoch 64/64\n",
      "67/67 [==============================] - 1s 21ms/step - loss: -0.0262\n",
      "At the end of episode 64 the total reward was : -18.0\n",
      "Epoch 65/65\n",
      "79/79 [==============================] - 2s 23ms/step - loss: -0.0148\n",
      "\n",
      "Epoch 00065: saving model to saveData/CNN/2021-05-31-22-30-05/0065.hdf5\n",
      "At the end of episode 65 the total reward was : -20.0\n",
      "Epoch 66/66\n",
      "64/64 [==============================] - 1s 21ms/step - loss: -0.0211\n",
      "At the end of episode 66 the total reward was : -21.0\n",
      "Epoch 67/67\n",
      "53/53 [==============================] - 1s 22ms/step - loss: 0.0017\n",
      "At the end of episode 67 the total reward was : -20.0\n",
      "Epoch 68/68\n",
      "56/56 [==============================] - 1s 25ms/step - loss: -0.0377\n",
      "At the end of episode 68 the total reward was : -20.0\n",
      "Epoch 69/69\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 0.0304\n",
      "At the end of episode 69 the total reward was : -21.0\n",
      "Epoch 70/70\n",
      "52/52 [==============================] - 1s 22ms/step - loss: 0.0564\n",
      "\n",
      "Epoch 00070: saving model to saveData/CNN/2021-05-31-22-30-05/0070.hdf5\n",
      "At the end of episode 70 the total reward was : -21.0\n",
      "Epoch 71/71\n",
      "60/60 [==============================] - 1s 21ms/step - loss: -0.0341\n",
      "At the end of episode 71 the total reward was : -19.0\n",
      "Epoch 72/72\n",
      "83/83 [==============================] - 2s 20ms/step - loss: -0.0053\n",
      "At the end of episode 72 the total reward was : -21.0\n",
      "Epoch 73/73\n",
      "85/85 [==============================] - 2s 23ms/step - loss: 0.0070\n",
      "At the end of episode 73 the total reward was : -20.0\n",
      "Epoch 74/74\n",
      "74/74 [==============================] - 2s 20ms/step - loss: -2.0801e-04\n",
      "At the end of episode 74 the total reward was : -18.0\n",
      "Epoch 75/75\n",
      "110/110 [==============================] - 8s 40ms/step - loss: -0.0502\n",
      "\n",
      "Epoch 00075: saving model to saveData/CNN/2021-05-31-22-30-05/0075.hdf5\n",
      "At the end of episode 75 the total reward was : -21.0\n",
      "Epoch 76/76\n",
      "38/38 [==============================] - 1s 25ms/step - loss: -0.0231\n",
      "At the end of episode 76 the total reward was : -21.0\n",
      "Epoch 77/77\n",
      "50/50 [==============================] - 1s 22ms/step - loss: -0.0032\n",
      "At the end of episode 77 the total reward was : -21.0\n",
      "Epoch 78/78\n",
      "40/40 [==============================] - 1s 24ms/step - loss: -0.0553\n",
      "At the end of episode 78 the total reward was : -21.0\n",
      "Epoch 79/79\n",
      "35/35 [==============================] - 1s 25ms/step - loss: -0.0338\n",
      "At the end of episode 79 the total reward was : -21.0\n",
      "Epoch 80/80\n",
      "33/33 [==============================] - 1s 27ms/step - loss: -0.3498\n",
      "\n",
      "Epoch 00080: saving model to saveData/CNN/2021-05-31-22-30-05/0080.hdf5\n",
      "At the end of episode 80 the total reward was : -21.0\n",
      "Epoch 81/81\n",
      "34/34 [==============================] - 1s 26ms/step - loss: -0.0187\n",
      "At the end of episode 81 the total reward was : -21.0\n",
      "Epoch 82/82\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0044\n",
      "At the end of episode 82 the total reward was : -21.0\n",
      "Epoch 83/83\n",
      "34/34 [==============================] - 1s 25ms/step - loss: -0.0315\n",
      "At the end of episode 83 the total reward was : -21.0\n",
      "Epoch 84/84\n",
      "35/35 [==============================] - 1s 38ms/step - loss: -0.0394\n",
      "At the end of episode 84 the total reward was : -21.0\n",
      "Epoch 85/85\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 0.0114\n",
      "\n",
      "Epoch 00085: saving model to saveData/CNN/2021-05-31-22-30-05/0085.hdf5\n",
      "At the end of episode 85 the total reward was : -21.0\n",
      "Epoch 86/86\n",
      "69/69 [==============================] - 1s 21ms/step - loss: -0.0389\n",
      "At the end of episode 86 the total reward was : -21.0\n",
      "Epoch 87/87\n",
      "60/60 [==============================] - 1s 21ms/step - loss: 0.0121\n",
      "At the end of episode 87 the total reward was : -21.0\n",
      "Epoch 88/88\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0022\n",
      "At the end of episode 88 the total reward was : -19.0\n",
      "Epoch 89/89\n",
      "84/84 [==============================] - 2s 19ms/step - loss: -0.0577\n",
      "At the end of episode 89 the total reward was : -21.0\n",
      "Epoch 90/90\n",
      "75/75 [==============================] - 2s 21ms/step - loss: -0.0129\n",
      "\n",
      "Epoch 00090: saving model to saveData/CNN/2021-05-31-22-30-05/0090.hdf5\n",
      "At the end of episode 90 the total reward was : -21.0\n",
      "Epoch 91/91\n",
      "85/85 [==============================] - 2s 21ms/step - loss: -0.1116\n",
      "At the end of episode 91 the total reward was : -21.0\n",
      "Epoch 92/92\n",
      "86/86 [==============================] - 2s 20ms/step - loss: -0.0420\n",
      "At the end of episode 92 the total reward was : -21.0\n",
      "Epoch 93/93\n",
      "86/86 [==============================] - 2s 20ms/step - loss: -0.2113\n",
      "At the end of episode 93 the total reward was : -21.0\n",
      "Epoch 94/94\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 0.0494\n",
      "At the end of episode 94 the total reward was : -21.0\n",
      "Epoch 95/95\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0208\n",
      "\n",
      "Epoch 00095: saving model to saveData/CNN/2021-05-31-22-30-05/0095.hdf5\n",
      "At the end of episode 95 the total reward was : -19.0\n",
      "Epoch 96/96\n",
      "70/70 [==============================] - 2s 21ms/step - loss: 0.0139\n",
      "At the end of episode 96 the total reward was : -21.0\n",
      "Epoch 97/97\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.0285\n",
      "At the end of episode 97 the total reward was : -19.0\n",
      "Epoch 98/98\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0217\n",
      "At the end of episode 98 the total reward was : -21.0\n",
      "Epoch 99/99\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.0074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-65f75539015a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# forward the policy network and sample action according to the proba distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mupProbability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mupProbability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUP_ACTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1704\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1955\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \"\"\"\n\u001b[0;32m-> 1957\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4573\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4574\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4575\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4576\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2097\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2098\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FlatMapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2100\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode = 0\n",
    "running_reward = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    upProbability = model.predict(np.array([observation]))[0]\n",
    "    if np.random.uniform() < upProbability:\n",
    "        action = UP_ACTION\n",
    "    else:\n",
    "        action = DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append([observation])\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    env.env.render()\n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode, 'the total reward was :', reward_sum)\n",
    "        \n",
    "        # training\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, callbacks=callbacks, sample_weight=dataManager.calculateRewards(rewards, gamma),initial_epoch = episode, epochs = episode+1)\n",
    "        \n",
    "        running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "        #Log Running Reward\n",
    "        tf.summary.scalar(\"Running Reward\", running_reward,step=episode)\n",
    "        tf.summary.scalar(\"Reward Sum\", reward_sum,step=episode)\n",
    "        \n",
    "        \n",
    "        # increment episode number\n",
    "        episode += 1\n",
    "        \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
